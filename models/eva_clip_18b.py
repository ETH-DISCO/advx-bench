# https://arxiv.org/pdf/2402.04252
# https://huggingface.co/papers/2402.04252


# from transformers import CLIPProcessor, CLIPModel

# model = CLIPModel.from_pretrained("BAAI/EVA-CLIP-18B")
# processor = CLIPProcessor.from_pretrained("BAAI/EVA-CLIP-18B")



# import torch
# from eva_clip import create_model_and_transforms, get_tokenizer

# model_name = "EVA-CLIP-18B"
# pretrained = "eva_clip"  # or path to downloaded weights
# model, _, processor = create_model_and_transforms(model_name, pretrained, force_custom_clip=True)

# tokenizer = get_tokenizer(model_name)
