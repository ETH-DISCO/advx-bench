# https://huggingface.co/mlunar/clip-variants/raw/555f7ba437324dd8e06b4e73fbd1605e6a0ba753/convert.py

# https://github.com/openai/CLIP
# https://github.com/wang-research-lab/roz/blob/main/scripts/natural_distribution_shift/src/models/CLIPViT.py

# import torchvision.models as models
# model = models.vit_b_16(pretrained=True)

